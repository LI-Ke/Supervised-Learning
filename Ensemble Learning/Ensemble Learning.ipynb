{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "#from sklearn.datasets import load_svmlight_file\n",
    "#X_train, y_train = load_svmlight_file(\"a7a\")\n",
    "#X_test, y_test = load_svmlight_file(\"a7a.t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = sklearn.datasets.make_classification(n_samples=20000,\n",
    "                                          n_features=30,    # 30 features\n",
    "                                          n_informative=5,  # only 5 informatives ones\n",
    "                                          n_redundant=0,\n",
    "                                          n_repeated=3,     # and 3 duplicate\n",
    "                                          n_classes=2,\n",
    "                                          n_clusters_per_class=1,\n",
    "                                          weights=None,\n",
    "                                          flip_y=0.03,\n",
    "                                          class_sep=0.8,\n",
    "                                          hypercube=True,\n",
    "                                          shift=0.0,\n",
    "                                          scale=1.0,\n",
    "                                          shuffle=True,\n",
    "                                          random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data= ds[0]\n",
    "Target= ds[1]\n",
    "XTrain=Data[:10000]\n",
    "YTrain=Target[:10000]\n",
    "XTest=Data[10000:]\n",
    "YTest=Target[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme du Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Bagging(object):\n",
    "    def __init__(self,nbr_stumps):\n",
    "        self.nbr_stumps = nbr_stumps\n",
    "        self.fb = {}\n",
    "        \n",
    "    def fit(self,data,target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        labels = list(set(self.target))\n",
    "        n = len(self.data)\n",
    "        for i in range(0,self.nbr_stumps):\n",
    "            data_tmp = []\n",
    "            target_tmp = []\n",
    "            for j in range(0,n):\n",
    "                index = np.random.randint(0,n)\n",
    "                data_tmp.append(self.data[index])\n",
    "                target_tmp.append(self.target[index])\n",
    "            clf = tree.DecisionTreeClassifier(max_depth=1)\n",
    "            clf = clf.fit(data_tmp, target_tmp) \n",
    "            output = clf.predict(inputs)\n",
    "            self.fb[i]=output\n",
    "        \n",
    "        m = len(inputs)\n",
    "        nbrLabels=len(labels)  \n",
    "        output = []\n",
    "        for indice in range(0,m):\n",
    "            compter_index = np.zeros(nbrLabels)\n",
    "            for l in range(0,self.nbr_stumps):\n",
    "                for indexL in range(0,nbrLabels):\n",
    "                    if self.fb[l][indice] == labels[indexL]:\n",
    "                        compter_index[indexL] += 1\n",
    "            maxIndex = max((v, i) for i, v in enumerate(compter_index))[1]\n",
    "            label = labels[maxIndex]\n",
    "            output.append(label)\n",
    "        return output\n",
    "\n",
    "    def score(self,prediction,target):\n",
    "        \"\"\"\n",
    "        length = len(prediction)\n",
    "        nbrCorrect = sum(i == j for i, j in zip(prediction, target))\n",
    "        precision = nbrCorrect/(length*1.0)\n",
    "        return precision\"\"\"\n",
    "        return np.mean(prediction == target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7449\n"
     ]
    }
   ],
   "source": [
    "b = Bagging(5)\n",
    "b.fit(XTrain, YTrain)\n",
    "outputs = b.predict(XTest)\n",
    "print b.score(outputs,YTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme du Boosting (AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Boosting(object):\n",
    "    def __init__(self,nbr_stumps):\n",
    "        self.nbr_stumps = nbr_stumps\n",
    "        self.weights = None\n",
    "        self.alpha = np.matrix(np.zeros((self.nbr_stumps, 1)))\n",
    "        self.func = list()\n",
    "        \n",
    "    def fit(self,data,target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.M = len(self.target)\n",
    "        self.weights = np.ones(self.M)/self.M\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        w = np.zeros(self.M)\n",
    "        for i in range(0,self.nbr_stumps):\n",
    "            clf = tree.DecisionTreeClassifier(max_depth=1)\n",
    "            clf = clf.fit(self.data, self.target) \n",
    "            output = clf.predict(self.data)\n",
    "            t = np.array(self.target)\n",
    "            p = np.array(output)\n",
    "            Indicator = np.abs(t-p)\n",
    "            errors=np.dot(Indicator,self.weights)\n",
    "            self.alpha[i]=0.5*np.log((1-errors)/errors)\n",
    "            for j in range(self.M):\n",
    "                if Indicator[j] == 1: \n",
    "                    w[j] = self.weights[j] * np.exp(self.alpha[i])\n",
    "                else: \n",
    "                    w[j] = self.weights[j] * np.exp(-self.alpha[i])\n",
    "            self.weights = w / w.sum()\n",
    "            self.func.append(clf)\n",
    "        predictions = []\n",
    "        f = np.zeros((1,len(inputs)))\n",
    "        for i in range(0,self.nbr_stumps):\n",
    "            f += self.alpha[i]*self.func[i].predict(inputs)\n",
    "        return np.sign(f)\n",
    "        \n",
    "    def score(self,prediction,target):\n",
    "        return np.mean(prediction == target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test du Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742\n"
     ]
    }
   ],
   "source": [
    "boosting = Boosting(5)\n",
    "boosting.fit(XTrain, YTrain)\n",
    "outputs = boosting.predict(XTest)\n",
    "print b.score(outputs,YTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion :\n",
    "Pour dresser une conclusion de nos expérimentations, plusieures affirmations peuvent être faites.\n",
    "Les algorithmes de Bagging et Boosting sont de bons moyens d'augmenter l'efficacité de son algorithme de classification faibles lorsqu'ils ont des précisions faibles.\n",
    "#### Comparaison entre les deux algorithmes :\n",
    "Le Bagging est plus simple à élaborer et peut être parrallélisé.\n",
    "Le Boosting atteind de meilleurs résultats. Ceci s'explique par l'utilisation de poids sur les exemples. Le poids des exemples mal classifiés est augmenté. Ce qui permet au prochain classifieur de ce concentrer sur les exemples d'entrainement mal classifiés."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
